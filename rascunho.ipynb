{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98f0832-b8d5-4554-978b-12b84a641718",
   "metadata": {},
   "source": [
    "# Rascunho contagem de veículos\n",
    "Para a câmera dos primeiros 23 minutos - https://www.youtube.com/watch?v=3uwRHtiyMnI\n",
    "\n",
    "Este Notebook contem o código que criei como \"rascunho\" para entender como resolver o problema e, ao mesmo tempo, uma possível solução que posteriormente foi aprimorada na versão seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5b04c2-0ddd-41ed-89b1-781b25adbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aefdc1-f432-4f01-9761-b7d710dc4018",
   "metadata": {},
   "source": [
    "O modelo pré-disponível do YOLO já possui as classes que preciso detectar, então vou aproveitar o modelo para o projeto. Somente para garantir um resultado melhor, irei utilizar a versão mais pesada que é o \"X\", neste momento não estou pensando em otimização, somente em validar a possibilidade de criar o sistema e criar as outras etapas que são a contagem em si (não quero me preocupar agora em anotar e treinar modelos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ba2a7e-6272-4c6a-8055-8554b7b44f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cd692b-d726-41b1-b709-f0e7dcea283f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'person',\n",
       " 1: 'bicycle',\n",
       " 2: 'car',\n",
       " 3: 'motorcycle',\n",
       " 4: 'airplane',\n",
       " 5: 'bus',\n",
       " 6: 'train',\n",
       " 7: 'truck',\n",
       " 8: 'boat',\n",
       " 9: 'traffic light',\n",
       " 10: 'fire hydrant',\n",
       " 11: 'stop sign',\n",
       " 12: 'parking meter',\n",
       " 13: 'bench',\n",
       " 14: 'bird',\n",
       " 15: 'cat',\n",
       " 16: 'dog',\n",
       " 17: 'horse',\n",
       " 18: 'sheep',\n",
       " 19: 'cow',\n",
       " 20: 'elephant',\n",
       " 21: 'bear',\n",
       " 22: 'zebra',\n",
       " 23: 'giraffe',\n",
       " 24: 'backpack',\n",
       " 25: 'umbrella',\n",
       " 26: 'handbag',\n",
       " 27: 'tie',\n",
       " 28: 'suitcase',\n",
       " 29: 'frisbee',\n",
       " 30: 'skis',\n",
       " 31: 'snowboard',\n",
       " 32: 'sports ball',\n",
       " 33: 'kite',\n",
       " 34: 'baseball bat',\n",
       " 35: 'baseball glove',\n",
       " 36: 'skateboard',\n",
       " 37: 'surfboard',\n",
       " 38: 'tennis racket',\n",
       " 39: 'bottle',\n",
       " 40: 'wine glass',\n",
       " 41: 'cup',\n",
       " 42: 'fork',\n",
       " 43: 'knife',\n",
       " 44: 'spoon',\n",
       " 45: 'bowl',\n",
       " 46: 'banana',\n",
       " 47: 'apple',\n",
       " 48: 'sandwich',\n",
       " 49: 'orange',\n",
       " 50: 'broccoli',\n",
       " 51: 'carrot',\n",
       " 52: 'hot dog',\n",
       " 53: 'pizza',\n",
       " 54: 'donut',\n",
       " 55: 'cake',\n",
       " 56: 'chair',\n",
       " 57: 'couch',\n",
       " 58: 'potted plant',\n",
       " 59: 'bed',\n",
       " 60: 'dining table',\n",
       " 61: 'toilet',\n",
       " 62: 'tv',\n",
       " 63: 'laptop',\n",
       " 64: 'mouse',\n",
       " 65: 'remote',\n",
       " 66: 'keyboard',\n",
       " 67: 'cell phone',\n",
       " 68: 'microwave',\n",
       " 69: 'oven',\n",
       " 70: 'toaster',\n",
       " 71: 'sink',\n",
       " 72: 'refrigerator',\n",
       " 73: 'book',\n",
       " 74: 'clock',\n",
       " 75: 'vase',\n",
       " 76: 'scissors',\n",
       " 77: 'teddy bear',\n",
       " 78: 'hair drier',\n",
       " 79: 'toothbrush'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517c3bc-e01a-4e85-8d02-e3d6ea2db112",
   "metadata": {},
   "source": [
    "Para facilitar minha vida, fiz a detecção de frames no vídeo até encontrar algum objeto para utilizar nas próximas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bfa587-2f58-4b8b-8584-15e432a330ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video = model.track(source='camera.mp4',  # image or video; single value or a list; URL, PIL (RGB), CV2 (BGR), ...\n",
    "                            conf=0.25,\n",
    "                            iou=0.7,\n",
    "                            imgsz=640,\n",
    "                            classes=[2, 3, 5, 7],\n",
    "                            show=False,\n",
    "                            save=True,\n",
    "                            save_txt=False,  # Save bbox coordination\n",
    "                            save_conf=False,  # save_txt must be True\n",
    "                            save_crop=False,\n",
    "                            verbose=False,\n",
    "                            stream=True  # Do inference now (False) or after (True)\n",
    "                           )\n",
    "for result in results_video:\n",
    "    if len(result.boxes.cls):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88e68c1-cabd-42ef-8d0d-d2c0ce21a7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 7., 2., 2., 2.])\n",
       "conf: tensor([0.8638, 0.8509, 0.7668, 0.7581, 0.7230, 0.7204, 0.6943, 0.6914, 0.6080, 0.5529, 0.4777, 0.4333, 0.3480, 0.2592, 0.2581])\n",
       "data: tensor([[9.5859e+02, 5.7417e+02, 1.0301e+03, 6.2251e+02, 1.0000e+00, 8.6379e-01, 2.0000e+00],\n",
       "        [4.4731e+02, 5.8712e+02, 5.1338e+02, 6.4473e+02, 2.0000e+00, 8.5087e-01, 2.0000e+00],\n",
       "        [4.8163e+02, 4.7221e+02, 5.2497e+02, 5.1029e+02, 3.0000e+00, 7.6677e-01, 2.0000e+00],\n",
       "        [7.1962e+02, 4.0680e+02, 7.4181e+02, 4.2482e+02, 4.0000e+00, 7.5809e-01, 2.0000e+00],\n",
       "        [5.1092e+02, 4.3721e+02, 5.4516e+02, 4.7007e+02, 5.0000e+00, 7.2302e-01, 2.0000e+00],\n",
       "        [5.7998e+02, 4.1678e+02, 6.0729e+02, 4.4239e+02, 6.0000e+00, 7.2041e-01, 2.0000e+00],\n",
       "        [5.4522e+02, 4.1599e+02, 5.6849e+02, 4.3348e+02, 7.0000e+00, 6.9435e-01, 2.0000e+00],\n",
       "        [5.2747e+02, 4.0326e+02, 5.4749e+02, 4.1909e+02, 8.0000e+00, 6.9138e-01, 2.0000e+00],\n",
       "        [7.6477e+02, 4.0746e+02, 7.8524e+02, 4.2341e+02, 9.0000e+00, 6.0801e-01, 2.0000e+00],\n",
       "        [4.7290e+02, 4.4249e+02, 5.0161e+02, 4.6833e+02, 1.0000e+01, 5.5295e-01, 2.0000e+00],\n",
       "        [7.1523e+02, 3.8111e+02, 7.3047e+02, 3.9276e+02, 1.1000e+01, 4.7773e-01, 2.0000e+00],\n",
       "        [8.1657e+02, 4.6075e+02, 8.5688e+02, 4.9539e+02, 1.2000e+01, 4.3326e-01, 7.0000e+00],\n",
       "        [7.5739e+02, 3.9734e+02, 7.7703e+02, 4.1208e+02, 1.3000e+01, 3.4804e-01, 2.0000e+00],\n",
       "        [7.4891e+02, 3.6008e+02, 7.6672e+02, 3.7366e+02, 1.4000e+01, 2.5923e-01, 2.0000e+00],\n",
       "        [5.4263e+02, 3.7879e+02, 5.6333e+02, 3.9671e+02, 1.5000e+01, 2.5808e-01, 2.0000e+00]])\n",
       "id: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.])\n",
       "is_track: True\n",
       "orig_shape: (720, 1280)\n",
       "shape: torch.Size([15, 7])\n",
       "xywh: tensor([[994.3623, 598.3425,  71.5405,  48.3409],\n",
       "        [480.3488, 615.9202,  66.0715,  57.6100],\n",
       "        [503.3014, 491.2515,  43.3336,  38.0853],\n",
       "        [730.7191, 415.8092,  22.1887,  18.0235],\n",
       "        [528.0439, 453.6409,  34.2422,  32.8529],\n",
       "        [593.6342, 429.5868,  27.3119,  25.6063],\n",
       "        [556.8528, 424.7363,  23.2686,  17.4900],\n",
       "        [537.4809, 411.1736,  20.0278,  15.8275],\n",
       "        [775.0063, 415.4349,  20.4761,  15.9579],\n",
       "        [487.2523, 455.4117,  28.7102,  25.8394],\n",
       "        [722.8491, 386.9374,  15.2319,  11.6476],\n",
       "        [836.7219, 478.0699,  40.3064,  34.6389],\n",
       "        [767.2130, 404.7097,  19.6431,  14.7316],\n",
       "        [757.8156, 366.8708,  17.8054,  13.5733],\n",
       "        [552.9789, 387.7493,  20.6956,  17.9276]])\n",
       "xywhn: tensor([[0.7768, 0.8310, 0.0559, 0.0671],\n",
       "        [0.3753, 0.8554, 0.0516, 0.0800],\n",
       "        [0.3932, 0.6823, 0.0339, 0.0529],\n",
       "        [0.5709, 0.5775, 0.0173, 0.0250],\n",
       "        [0.4125, 0.6301, 0.0268, 0.0456],\n",
       "        [0.4638, 0.5966, 0.0213, 0.0356],\n",
       "        [0.4350, 0.5899, 0.0182, 0.0243],\n",
       "        [0.4199, 0.5711, 0.0156, 0.0220],\n",
       "        [0.6055, 0.5770, 0.0160, 0.0222],\n",
       "        [0.3807, 0.6325, 0.0224, 0.0359],\n",
       "        [0.5647, 0.5374, 0.0119, 0.0162],\n",
       "        [0.6537, 0.6640, 0.0315, 0.0481],\n",
       "        [0.5994, 0.5621, 0.0153, 0.0205],\n",
       "        [0.5920, 0.5095, 0.0139, 0.0189],\n",
       "        [0.4320, 0.5385, 0.0162, 0.0249]])\n",
       "xyxy: tensor([[ 958.5920,  574.1720, 1030.1326,  622.5129],\n",
       "        [ 447.3130,  587.1152,  513.3845,  644.7252],\n",
       "        [ 481.6346,  472.2089,  524.9681,  510.2942],\n",
       "        [ 719.6248,  406.7974,  741.8135,  424.8209],\n",
       "        [ 510.9228,  437.2145,  545.1650,  470.0674],\n",
       "        [ 579.9782,  416.7837,  607.2901,  442.3899],\n",
       "        [ 545.2185,  415.9913,  568.4871,  433.4813],\n",
       "        [ 527.4670,  403.2599,  547.4948,  419.0873],\n",
       "        [ 764.7683,  407.4559,  785.2444,  423.4138],\n",
       "        [ 472.8972,  442.4920,  501.6075,  468.3313],\n",
       "        [ 715.2332,  381.1136,  730.4651,  392.7612],\n",
       "        [ 816.5687,  460.7505,  856.8751,  495.3894],\n",
       "        [ 757.3915,  397.3439,  777.0345,  412.0755],\n",
       "        [ 748.9129,  360.0841,  766.7183,  373.6574],\n",
       "        [ 542.6311,  378.7855,  563.3267,  396.7131]])\n",
       "xyxyn: tensor([[0.7489, 0.7975, 0.8048, 0.8646],\n",
       "        [0.3495, 0.8154, 0.4011, 0.8955],\n",
       "        [0.3763, 0.6558, 0.4101, 0.7087],\n",
       "        [0.5622, 0.5650, 0.5795, 0.5900],\n",
       "        [0.3992, 0.6072, 0.4259, 0.6529],\n",
       "        [0.4531, 0.5789, 0.4744, 0.6144],\n",
       "        [0.4260, 0.5778, 0.4441, 0.6021],\n",
       "        [0.4121, 0.5601, 0.4277, 0.5821],\n",
       "        [0.5975, 0.5659, 0.6135, 0.5881],\n",
       "        [0.3695, 0.6146, 0.3919, 0.6505],\n",
       "        [0.5588, 0.5293, 0.5707, 0.5455],\n",
       "        [0.6379, 0.6399, 0.6694, 0.6880],\n",
       "        [0.5917, 0.5519, 0.6071, 0.5723],\n",
       "        [0.5851, 0.5001, 0.5990, 0.5190],\n",
       "        [0.4239, 0.5261, 0.4401, 0.5510]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116d56bb-4a1e-4e7b-8545-80c4c97a957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('img_original.png', result.orig_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357abc1-3c7f-4678-af16-eae03cc15734",
   "metadata": {},
   "source": [
    "Neste caso decidi utilizar o centro de cada box para usar como referência de onde o objeto se encontra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb30e73-a1b0-4b4b-a0c5-ea072567af2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 598)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_x = int((result.boxes.xyxy[0][0] + result.boxes.xyxy[0][2]) / 2)\n",
    "center_y = int((result.boxes.xyxy[0][1] + result.boxes.xyxy[0][3]) / 2)\n",
    "center_x, center_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570992a-ca4e-4a39-831b-2959289db151",
   "metadata": {},
   "source": [
    "Identificando as melhores posições e coordenadas para botar as linhas de contagem e para criar o painel de contagem. Para facilitar estou desenhando na imagem (mais fácil de debugar), mas em si, será utilizado somente as coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5bcf56f-7649-428b-bc76-21a77afeaf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('img_original.png')\n",
    "cv2.line(img, (649, 590), (180, 590), (0, 0, 255), 3)  # Left\n",
    "cv2.line(img, (650, 531), (1023, 531), (255, 0, 0), 3)  # Right\n",
    "cv2.line(img, (650, 1280), (650, 0), (255, 0, 255), 3)  # Division\n",
    "cv2.circle(img, (994,597), color=(255, 255, 0), radius=1, thickness=10)\n",
    "\n",
    "# White board (results)\n",
    "cv2.rectangle(img, (10, 10), (230, 95), (255, 255, 255), thickness=-1)\n",
    "cv2.rectangle(img, (10, 10), (230, 95), (0, 0, 0), thickness=2)\n",
    "cv2.putText(img, f'Left: 0', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "cv2.putText(img, f'Right: 0', (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "cv2.imwrite('img_lines.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205d8c-52dc-4229-81c5-967e8d2a80c1",
   "metadata": {},
   "source": [
    "Código inicial da contagem. Simplifiquei, fazer uma linha de corte vertical separando as pistas, possibilitando reconhecer se o objeto está na esquerda ou direita para saber qual linha de contagem irei utilizar. Também adicionei um tratamento para evitar contar o mesmo objeto mais de uma vez e um buffer, o qual o tracker ID precisa ficar sem ser detectado por um tempo até considerar que não está mais presente na imagem (isso, pois o objeto pode ficar sem ser detectar por alguma falha ou obstrução por um curto período... algo normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31635a9-99e5-4376-a5ec-e881784e180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {'left': 0, 'right': 0}\n",
    "detections = {}\n",
    "side = 'left' if center_x < 650 else 'right'\n",
    "if side == 'left':\n",
    "    position_y = 'up' if center_y < 590 else 'down'\n",
    "else:\n",
    "    position_y = 'up' if center_y < 531 else 'down'\n",
    "\n",
    "track_id = int(result.boxes.id[0])\n",
    "# Track_id already detected\n",
    "if track_id in detections:\n",
    "    detections[track_id]['count_not_detected'] = 0\n",
    "    if detections[track_id]['position_y'] != position_y:\n",
    "        # Only consider those who crossed the line at the correct position_y, otherwise, treat it as a mistake and ignore it\n",
    "        if side == 'left' and position_y == 'down' or side == 'right' and position_y == 'up':\n",
    "            counter[side] += 1\n",
    "            detections[track_id]['position_y'] = position_y\n",
    "# New track_id\n",
    "else:\n",
    "    detections[track_id] = {'count_not_detected': 0, 'side': side, 'position_y': position_y}\n",
    "\n",
    "# Remove any detections that no longer exist for N consecutive frames\n",
    "for key in list(detections.keys()):\n",
    "    detections[key]['count_not_detected'] += 1\n",
    "    if detections[key]['count_not_detected'] >= 20:\n",
    "        del detections[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36c4bc0-5c56-4b83-9843-1b6ea094c7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'count_not_detected': 1, 'side': 'right', 'position_y': 'down'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
